{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filling rows that contain missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/abhi/spark-2.2.1-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('preprocess').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('/home/abhi/project/orange_small_train.data',inferSchema=True,header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "null_count=df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## column-wise null count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Var1=49298, Var2=48759, Var3=48760, Var4=48421, Var5=48513, Var6=5529, Var7=5539, Var8=50000, Var9=49298, Var10=48513, Var11=48760, Var12=49442, Var13=5539, Var14=48760, Var15=50000, Var16=48513, Var17=48421, Var18=48421, Var19=48421, Var20=50000, Var21=5529, Var22=5009, Var23=48513, Var24=7230, Var25=5009, Var26=48513, Var27=48513, Var28=5011, Var29=49298, Var30=49298, Var31=50000, Var32=50000, Var33=49153, Var34=48759, Var35=5009, Var36=48759, Var37=48421, Var38=5009, Var39=50000, Var40=48759, Var41=49298, Var42=50000, Var43=48759, Var44=5009, Var45=49656, Var46=48759, Var47=49298, Var48=50000, Var49=48759, Var50=49298, Var51=46253, Var52=50000, Var53=49298, Var54=48759, Var55=50000, Var56=49354, Var57=0, Var58=49298, Var59=49180, Var60=48513, Var61=49153, Var62=49442, Var63=49306, Var64=49762, Var65=5539, Var66=49306, Var67=48513, Var68=48759, Var69=48513, Var70=48513, Var71=48871, Var72=22380, Var73=0, Var74=5539, Var75=48759, Var76=5009, Var77=49298, Var78=5009, Var79=50000, Var80=48513, Var81=5529, Var82=48421, Var83=5009, Var84=48760, Var85=5009, Var86=49298, Var87=49298, Var88=48917, Var89=49354, Var90=49298, Var91=48871, Var92=49829, Var93=48513, Var94=22380, Var95=48759, Var96=48759, Var97=48513, Var98=49442, Var99=48421, Var100=49298, Var101=49127, Var102=49549, Var103=48513, Var104=49180, Var105=49180, Var106=48421, Var107=48513, Var108=49298, Var109=7230, Var110=49298, Var111=48871, Var112=5009, Var113=0, Var114=48759, Var115=49180, Var116=49298, Var117=48421, Var118=49829, Var119=5529, Var120=48513, Var121=49298, Var122=48759, Var123=5009, Var124=48421, Var125=5539, Var126=13920, Var127=48917, Var128=48917, Var129=49298, Var130=48760, Var131=49298, Var132=5009, Var133=5009, Var134=5009, Var135=48421, Var136=49306, Var137=49298, Var138=48421, Var139=48513, Var140=5539, Var141=50000, Var142=49298, Var143=5009, Var144=5529, Var145=48421, Var146=48513, Var147=48513, Var148=48513, Var149=7230, Var150=48421, Var151=49153, Var152=48421, Var153=5009, Var154=49298, Var155=48421, Var156=49306, Var157=48871, Var158=49127, Var159=48759, Var160=5009, Var161=48421, Var162=48759, Var163=5009, Var164=48421, Var165=49127, Var166=48513, Var167=50000, Var168=49298, Var169=50000, Var170=48759, Var171=48917, Var172=48513, Var173=5009, Var174=48421, Var175=50000, Var176=48760, Var177=48759, Var178=49354, Var179=48421, Var180=49298, Var181=5009, Var182=48421, Var183=48759, Var184=48759, Var185=50000, Var186=49298, Var187=49298, Var188=48759, Var189=28978, Var190=49667, Var191=48917, Var192=369, Var193=0, Var194=37216, Var195=0, Var196=0, Var197=143, Var198=0, Var199=4, Var200=25408, Var201=37217, Var202=1, Var203=143, Var204=0, Var205=1934, Var206=5529, Var207=0, Var208=143, Var209=50000, Var210=0, Var211=0, Var212=0, Var213=48871, Var214=25408, Var215=49306, Var216=0, Var217=703, Var218=703, Var219=5211, Var220=0, Var221=0, Var222=0, Var223=5211, Var224=49180, Var225=26144, Var226=0, Var227=0, Var228=0, Var229=28432, Var230=50000)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting threshold for null values of a column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list=[]\n",
    "thresh=0.60*df.count()\n",
    "i=0\n",
    "while i<len(df.columns):\n",
    "    if(null_count[0][i]<thresh):\n",
    "        my_list.append(\"Var{}\".format(i+1))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of columns that are below the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Var6',\n",
       " 'Var7',\n",
       " 'Var13',\n",
       " 'Var21',\n",
       " 'Var22',\n",
       " 'Var24',\n",
       " 'Var25',\n",
       " 'Var28',\n",
       " 'Var35',\n",
       " 'Var38',\n",
       " 'Var44',\n",
       " 'Var57',\n",
       " 'Var65',\n",
       " 'Var72',\n",
       " 'Var73',\n",
       " 'Var74',\n",
       " 'Var76',\n",
       " 'Var78',\n",
       " 'Var81',\n",
       " 'Var83',\n",
       " 'Var85',\n",
       " 'Var94',\n",
       " 'Var109',\n",
       " 'Var112',\n",
       " 'Var113',\n",
       " 'Var119',\n",
       " 'Var123',\n",
       " 'Var125',\n",
       " 'Var126',\n",
       " 'Var132',\n",
       " 'Var133',\n",
       " 'Var134',\n",
       " 'Var140',\n",
       " 'Var143',\n",
       " 'Var144',\n",
       " 'Var149',\n",
       " 'Var153',\n",
       " 'Var160',\n",
       " 'Var163',\n",
       " 'Var173',\n",
       " 'Var181',\n",
       " 'Var189',\n",
       " 'Var192',\n",
       " 'Var193',\n",
       " 'Var195',\n",
       " 'Var196',\n",
       " 'Var197',\n",
       " 'Var198',\n",
       " 'Var199',\n",
       " 'Var200',\n",
       " 'Var202',\n",
       " 'Var203',\n",
       " 'Var204',\n",
       " 'Var205',\n",
       " 'Var206',\n",
       " 'Var207',\n",
       " 'Var208',\n",
       " 'Var210',\n",
       " 'Var211',\n",
       " 'Var212',\n",
       " 'Var214',\n",
       " 'Var216',\n",
       " 'Var217',\n",
       " 'Var218',\n",
       " 'Var219',\n",
       " 'Var220',\n",
       " 'Var221',\n",
       " 'Var222',\n",
       " 'Var223',\n",
       " 'Var225',\n",
       " 'Var226',\n",
       " 'Var227',\n",
       " 'Var228',\n",
       " 'Var229']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selecting the columns that are below the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cols=df.select(my_list)\n",
    "my_cols.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identifying the type of data present in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list=[]\n",
    "cat_list=[]\n",
    "for name,dtype in my_cols.dtypes:\n",
    "    if(dtype=='int' or dtype=='double' or dtype=='float'):\n",
    "        num_list.append(name)\n",
    "    else:\n",
    "        cat_list.append(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filling the missing values of numeric data type columns with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in num_list:\n",
    "        average=my_cols.select(mean(c)).collect()\n",
    "        my_cols = my_cols.fillna(average[0][0],c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filling the missing values of categorical data with mode\n",
    "## frequently repeated value is placed in missing values of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_list:\n",
    "    freq_count=my_cols.groupBy(c).count()\n",
    "    freq_list=freq_count.orderBy(freq_count[\"count\"].desc()).collect()\n",
    "    if(freq_list[0][0]==None):\n",
    "        my_cols = my_cols.fillna(freq_list[1][0],c)\n",
    "    else:\n",
    "        my_cols = my_cols.fillna(freq_list[0][0],c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total rows after filling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cols.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count2=my_cols.select([count(when(isnull(c), c)).alias(c) for c in my_cols.columns]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nullcount of each column after filling missing values(which is  zero for every column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Var6=0, Var7=0, Var13=0, Var21=0, Var22=0, Var24=0, Var25=0, Var28=0, Var35=0, Var38=0, Var44=0, Var57=0, Var65=0, Var72=0, Var73=0, Var74=0, Var76=0, Var78=0, Var81=0, Var83=0, Var85=0, Var94=0, Var109=0, Var112=0, Var113=0, Var119=0, Var123=0, Var125=0, Var126=0, Var132=0, Var133=0, Var134=0, Var140=0, Var143=0, Var144=0, Var149=0, Var153=0, Var160=0, Var163=0, Var173=0, Var181=0, Var189=0, Var192=0, Var193=0, Var195=0, Var196=0, Var197=0, Var198=0, Var199=0, Var200=0, Var202=0, Var203=0, Var204=0, Var205=0, Var206=0, Var207=0, Var208=0, Var210=0, Var211=0, Var212=0, Var214=0, Var216=0, Var217=0, Var218=0, Var219=0, Var220=0, Var221=0, Var222=0, Var223=0, Var225=0, Var226=0, Var227=0, Var228=0, Var229=0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_label = spark.read.csv('/home/abhi/project/orange_small_train_churn.labels',inferSchema=True,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using lit() to add a new column that only '1' in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "my_cols = my_cols.withColumn(\"order\",lit(1))\n",
    "churn_label = churn_label.withColumn(\"order\",lit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating another column rowNum that contains consecutive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "my_cols = my_cols.select(\"*\", F.row_number().over(Window.partitionBy(\"order\").orderBy(\"order\")).alias(\"rowNum\"))\n",
    "churn_label = churn_label.select(\"*\", F.row_number().over(Window.partitionBy(\"order\").orderBy(\"order\")).alias(\"rowNum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## joining the feature columns data frame with label dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_cols.join(churn_label,my_cols.rowNum == churn_label.rowNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema of the resultant dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Var6: integer (nullable = true)\n",
      " |-- Var7: integer (nullable = true)\n",
      " |-- Var13: integer (nullable = true)\n",
      " |-- Var21: integer (nullable = true)\n",
      " |-- Var22: integer (nullable = true)\n",
      " |-- Var24: integer (nullable = true)\n",
      " |-- Var25: integer (nullable = true)\n",
      " |-- Var28: double (nullable = false)\n",
      " |-- Var35: integer (nullable = true)\n",
      " |-- Var38: integer (nullable = true)\n",
      " |-- Var44: integer (nullable = true)\n",
      " |-- Var57: double (nullable = false)\n",
      " |-- Var65: integer (nullable = true)\n",
      " |-- Var72: integer (nullable = true)\n",
      " |-- Var73: integer (nullable = true)\n",
      " |-- Var74: integer (nullable = true)\n",
      " |-- Var76: integer (nullable = true)\n",
      " |-- Var78: integer (nullable = true)\n",
      " |-- Var81: double (nullable = false)\n",
      " |-- Var83: integer (nullable = true)\n",
      " |-- Var85: integer (nullable = true)\n",
      " |-- Var94: integer (nullable = true)\n",
      " |-- Var109: integer (nullable = true)\n",
      " |-- Var112: integer (nullable = true)\n",
      " |-- Var113: double (nullable = false)\n",
      " |-- Var119: integer (nullable = true)\n",
      " |-- Var123: integer (nullable = true)\n",
      " |-- Var125: integer (nullable = true)\n",
      " |-- Var126: integer (nullable = true)\n",
      " |-- Var132: integer (nullable = true)\n",
      " |-- Var133: integer (nullable = true)\n",
      " |-- Var134: integer (nullable = true)\n",
      " |-- Var140: integer (nullable = true)\n",
      " |-- Var143: integer (nullable = true)\n",
      " |-- Var144: integer (nullable = true)\n",
      " |-- Var149: integer (nullable = true)\n",
      " |-- Var153: integer (nullable = true)\n",
      " |-- Var160: integer (nullable = true)\n",
      " |-- Var163: integer (nullable = true)\n",
      " |-- Var173: integer (nullable = true)\n",
      " |-- Var181: integer (nullable = true)\n",
      " |-- Var189: integer (nullable = true)\n",
      " |-- Var192: string (nullable = false)\n",
      " |-- Var193: string (nullable = false)\n",
      " |-- Var195: string (nullable = false)\n",
      " |-- Var196: string (nullable = false)\n",
      " |-- Var197: string (nullable = false)\n",
      " |-- Var198: string (nullable = false)\n",
      " |-- Var199: string (nullable = false)\n",
      " |-- Var200: string (nullable = false)\n",
      " |-- Var202: string (nullable = false)\n",
      " |-- Var203: string (nullable = false)\n",
      " |-- Var204: string (nullable = false)\n",
      " |-- Var205: string (nullable = false)\n",
      " |-- Var206: string (nullable = false)\n",
      " |-- Var207: string (nullable = false)\n",
      " |-- Var208: string (nullable = false)\n",
      " |-- Var210: string (nullable = false)\n",
      " |-- Var211: string (nullable = false)\n",
      " |-- Var212: string (nullable = false)\n",
      " |-- Var214: string (nullable = false)\n",
      " |-- Var216: string (nullable = false)\n",
      " |-- Var217: string (nullable = false)\n",
      " |-- Var218: string (nullable = false)\n",
      " |-- Var219: string (nullable = false)\n",
      " |-- Var220: string (nullable = false)\n",
      " |-- Var221: string (nullable = false)\n",
      " |-- Var222: string (nullable = false)\n",
      " |-- Var223: string (nullable = false)\n",
      " |-- Var225: string (nullable = false)\n",
      " |-- Var226: string (nullable = false)\n",
      " |-- Var227: string (nullable = false)\n",
      " |-- Var228: string (nullable = false)\n",
      " |-- Var229: string (nullable = false)\n",
      " |-- order: integer (nullable = false)\n",
      " |-- rowNum: integer (nullable = true)\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- order: integer (nullable = false)\n",
      " |-- rowNum: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping order and rowNum columns which are used to join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop(\"order\",\"rowNum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming the _c0 column to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.withColumnRenamed(\"_c0\",\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating the csv file for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.repartition(1).write.csv('/home/abhi/project/cleaned_data2',sep=',',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
